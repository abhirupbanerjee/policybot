services:
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy
    ports:
      - "4000:4000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    env_file:
      - ../.env.local  # Use root .env.local - single source of truth
    environment:
      # Override specific settings (env_file provides API keys)
      - STORE_MODEL_IN_DB=False
      - DISABLE_SPEND_LOGS=True
      # Clear OPENAI_BASE_URL so LiteLLM uses default OpenAI endpoint
      # (otherwise it would call itself in a loop)
      - OPENAI_BASE_URL=
    command: --config /app/config.yaml --port 4000
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health", "-H", "Authorization: Bearer $${LITELLM_MASTER_KEY}"]
      interval: 30s
      timeout: 10s
      retries: 3
