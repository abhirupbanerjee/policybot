{
  "$schema": "./defaults.schema.json",
  "version": "1.0",

  "modelPresets": {
    "gpt-4.1": {
      "name": "GPT-4.1 (High Performance)",
      "description": "Most capable OpenAI model with 1M context for complex policy analysis",
      "provider": "openai",
      "temperature": 0.1,
      "maxTokens": 4000,
      "topKChunks": 25,
      "maxContextChunks": 20,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    },
    "gpt-4.1-mini": {
      "name": "GPT-4.1 Mini (Balanced)",
      "description": "Fast and affordable for most policy queries with good accuracy",
      "provider": "openai",
      "temperature": 0.2,
      "maxTokens": 3000,
      "topKChunks": 20,
      "maxContextChunks": 15,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    },
    "gpt-4.1-nano": {
      "name": "GPT-4.1 Nano (Cost-Effective)",
      "description": "Cost-effective option for simpler queries with faster response times",
      "provider": "openai",
      "temperature": 0.2,
      "maxTokens": 2000,
      "topKChunks": 15,
      "maxContextChunks": 10,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    },
    "mistral-large-3": {
      "name": "Mistral Large 3",
      "description": "Mistral flagship model with 256K context and strong reasoning",
      "provider": "mistral",
      "temperature": 0.2,
      "maxTokens": 3000,
      "topKChunks": 20,
      "maxContextChunks": 15,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    },
    "mistral-small-3.2": {
      "name": "Mistral Small 3.2 (Cost-Effective)",
      "description": "Fast and efficient Mistral model for routine queries",
      "provider": "mistral",
      "temperature": 0.2,
      "maxTokens": 2000,
      "topKChunks": 15,
      "maxContextChunks": 10,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    },
    "gemini-2.5-pro": {
      "name": "Gemini 2.5 Pro",
      "description": "Google's flagship reasoning model with 1M context and thinking capabilities",
      "provider": "gemini",
      "temperature": 0.2,
      "maxTokens": 4000,
      "topKChunks": 25,
      "maxContextChunks": 20,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    },
    "gemini-2.5-flash": {
      "name": "Gemini 2.5 Flash (Balanced)",
      "description": "Fast hybrid reasoning model with 1M context - excellent price/performance",
      "provider": "gemini",
      "temperature": 0.2,
      "maxTokens": 3000,
      "topKChunks": 20,
      "maxContextChunks": 15,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    },
    "gemini-2.5-flash-lite": {
      "name": "Gemini 2.5 Flash-Lite (Cost-Effective)",
      "description": "Lowest cost Gemini model for high-volume simple queries",
      "provider": "gemini",
      "temperature": 0.2,
      "maxTokens": 2000,
      "topKChunks": 15,
      "maxContextChunks": 10,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    },
    "ollama-llama3.2": {
      "name": "Ollama Llama 3.2 (Local)",
      "description": "Local model with full tool support, no API cost",
      "provider": "ollama",
      "temperature": 0.2,
      "maxTokens": 2000,
      "topKChunks": 15,
      "maxContextChunks": 10,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    },
    "ollama-qwen2.5": {
      "name": "Ollama Qwen 2.5 (Local)",
      "description": "High-quality local model with excellent reasoning",
      "provider": "ollama",
      "temperature": 0.2,
      "maxTokens": 2000,
      "topKChunks": 15,
      "maxContextChunks": 10,
      "similarityThreshold": 0.5,
      "chunkSize": 1200,
      "chunkOverlap": 200,
      "queryExpansionEnabled": true,
      "cacheEnabled": true,
      "cacheTTLSeconds": 3600
    }
  },

  "defaultPreset": "gpt-4.1-mini",

  "llm": {
    "model": "gpt-4.1-mini",
    "temperature": 0.2,
    "maxTokens": 2000
  },

  "rag": {
    "topKChunks": 15,
    "maxContextChunks": 10,
    "similarityThreshold": 0.5,
    "chunkSize": 1200,
    "chunkOverlap": 200,
    "queryExpansionEnabled": true,
    "cacheEnabled": true,
    "cacheTTLSeconds": 3600
  },

  "embedding": {
    "model": "text-embedding-3-large",
    "dimensions": 3072
  },

  "reranker": {
    "enabled": false,
    "provider": "cohere",
    "topKForReranking": 50,
    "minRerankerScore": 0.3,
    "cacheTTLSeconds": 3600
  },

  "tavily": {
    "enabled": false,
    "defaultTopic": "general",
    "defaultSearchDepth": "basic",
    "maxResults": 5,
    "includeDomains": [],
    "excludeDomains": [],
    "cacheTTLSeconds": 3600
  },

  "upload": {
    "maxFilesPerInput": 5,
    "maxFilesPerThread": 10,
    "maxFileSizeMB": 10,
    "allowedTypes": [
      "application/pdf",
      "image/png",
      "image/jpeg",
      "text/plain"
    ]
  },

  "retention": {
    "threadRetentionDays": 90,
    "storageAlertThreshold": 70
  },

  "memory": {
    "enabled": false,
    "extractionThreshold": 5,
    "maxFactsPerCategory": 20,
    "autoExtractOnThreadEnd": true
  },

  "summarization": {
    "enabled": false,
    "tokenThreshold": 100000,
    "keepRecentMessages": 10,
    "summaryMaxTokens": 2000,
    "archiveOriginalMessages": true
  },

  "branding": {
    "botName": "Grenada AI Assistant",
    "botIcon": "government"
  },

  "limits": {
    "conversationHistoryMessages": 5,
    "maxQueryExpansions": 3,
    "userDocumentChunks": 5,
    "embeddingBatchSize": 100,
    "maxFilenameLength": 200,
    "toolCallMaxIterations": 3
  },

  "models": {
    "toolCapable": [
      "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano", "gpt-3.5-turbo",
      "mistral-large-3", "mistral-medium-3.1", "mistral-small-3.2",
      "gemini-2.5-pro", "gemini-2.5-flash", "gemini-2.5-flash-lite",
      "ollama-llama3.2", "ollama-llama3.1", "ollama-mistral", "ollama-qwen2.5"
    ],
    "transcription": "whisper-1",
    "rerankerCohere": "rerank-english-v3.0",
    "rerankerLocal": "Xenova/all-MiniLM-L6-v2"
  },

  "acronyms": {
    "ea": "enterprise architecture",
    "dta": "digital transformation agency",
    "it": "information technology",
    "ict": "information and communication technology",
    "hr": "human resources",
    "kpi": "key performance indicator",
    "sla": "service level agreement"
  }
}
