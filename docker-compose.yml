services:
  traefik:
    image: traefik:v3.0
    container_name: policy-bot-traefik
    command:
      - "--api.insecure=false"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.web.http.redirections.entryPoint.to=websecure"
      - "--entrypoints.web.http.redirections.entryPoint.scheme=https"
      - "--certificatesresolvers.letsencrypt.acme.tlschallenge=true"
      - "--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - letsencrypt:/letsencrypt
    networks:
      - policy-bot-network
    restart: unless-stopped

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: policy-bot-app
    environment:
      - NODE_ENV=production
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000
      - REDIS_URL=redis://redis:6379
      - DATA_DIR=/app/data
      - CONFIG_DIR=/app/config
      # Point to LiteLLM proxy for multi-provider LLM support
      - OPENAI_BASE_URL=http://litellm:4000/v1
    env_file:
      - .env
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.app.rule=Host(`${DOMAIN}`)"
      - "traefik.http.routers.app.entrypoints=websecure"
      - "traefik.http.routers.app.tls.certresolver=letsencrypt"
      - "traefik.http.services.app.loadbalancer.server.port=3000"
    volumes:
      - ./data/app:/app/data
      - ./config:/app/config:ro
    depends_on:
      chroma:
        condition: service_started
      redis:
        condition: service_healthy
      litellm:
        condition: service_healthy
    networks:
      - policy-bot-network
    restart: unless-stopped

  chroma:
    image: chromadb/chroma:latest
    container_name: policy-bot-chroma
    volumes:
      - ./data/chroma:/data
    environment:
      - ANONYMIZED_TELEMETRY=false
      - ALLOW_RESET=false
      - IS_PERSISTENT=TRUE
    networks:
      - policy-bot-network
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: policy-bot-redis
    volumes:
      - ./data/redis:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - policy-bot-network
    restart: unless-stopped

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: policy-bot-litellm
    volumes:
      - ./litellm-proxy/litellm_config.yaml:/app/config.yaml:ro
    env_file:
      - .env
    environment:
      # Clear OPENAI_BASE_URL so LiteLLM uses direct OpenAI endpoint (prevents loop)
      - OPENAI_BASE_URL=
      - STORE_MODEL_IN_DB=False
      - DISABLE_SPEND_LOGS=True
    command: --config /app/config.yaml --port 4000
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4000/health/liveliness')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - policy-bot-network
    restart: unless-stopped

networks:
  policy-bot-network:
    name: policy-bot-network

volumes:
  letsencrypt:
    name: policy-bot-letsencrypt
